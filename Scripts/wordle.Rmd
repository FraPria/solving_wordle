---
title: "Wordle"
output: github_document
  # html_document:
  #   toc: true
  #   toc_float: true
  #   toc_collapsed: false
  #   toc_depth: 3
  #   number_sections: false
---

<style>
    body {
        line-height: 1.5em;
        text-align: justify
    }
</style>


```{r setup, include=FALSE}
# https://gist.github.com/JoshuaTPierce/b919168421b40e06481080eb53c3fb2f
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(data.table)
library(stringr)
library(ComplexHeatmap)
library(viridis)
library(circlize)
library(dplyr)
library(patchwork)
library(ggplotify)
setwd('~/Desktop/coding_shit/solving_wordle/')
source('Scripts/config.R')


```

```{r example, include=T, echo = F, fig.height=1, fig.width=5, fig.align="center"}
plotGuess('wordle', getColors('wordle','wonder',6), 6 ) +
  theme(rect = element_rect(fill = "transparent"))
```

# Starting data

Wordle is an online game designed by Josh Wardle (no jokes). The goal is to guess a 5 letters word, given 3 types of clues derived by the game:  

* right letter in the right place (green).
* right letter but in the wrong place (yellow).
* the letter is not in the target word (grey).

For semplicity (and for limited computational resources) I will use only the 2315 [target words](https://docs.google.com/spreadsheets/d/1-M0RIVVZqbeh0mZacdAsJyBrLuEmhKUhNaVAI-7pr2Y/edit#gid=0) and not all the possible 5 letters words in the english dictionary.
The heatmap shows the frequency of each letter in each of the 5 positions. 

```{r dictionary, include=T, echo = F, fig.height=3, fig.width=7, fig.align="center"}
dictionary = read.delim('Data/solutions.txt', header = F)
rownames(dictionary) = dictionary$V1
dictionary1 = as.data.frame(sapply(1:5, function(i) { substr(dictionary$V1,i,i) } ))
alphabet = unique(unlist(c(dictionary1)))
freq = as.data.frame(sapply(colnames(dictionary1), function(pos) sapply(alphabet, function(letter) sum(dictionary1[,pos] ==letter))))


ordering = sort(rowSums(freq), decreasing = T)
names(ordering) = toupper(names(ordering))
mat = t(as.matrix(freq))
colnames(mat) = toupper(colnames(mat))
rownames(mat) = c('1','2','3','4','5')

mat = mat[,names(ordering)]
mat = mat[c('5','4','3','2','1'),]

ha = HeatmapAnnotation(frequency = anno_barplot(colSums(mat)))

col = colorRamp2(seq(min(mat), max(mat), length.out = 5),c(viridis(5)))
Heatmap(mat, col = col,
        top_annotation = ha,
        cluster_rows = F, 
        cluster_columns = F,
        heatmap_legend_param = list(title = ""),
        height              = unit(5, "mm")*nrow(mat),
        width               = unit(5, "mm")*ncol(mat),
        column_names_rot = 0)

```


# Strategy intuition

When we try to guess the target word we have $3^5$, so 243, possible colors configuration, which is less that the dimension of the dictionary (2315). This means that given a guess, some color configurations will be repeated for different target words.  \
Let's take for example the word CIGAR: it will have, for example:

* only 1 green-green-green-green-green configuration (when the target is CIGAR itself).
* 3 green-green-grey-grey-grey with CIVIC, CINCH and CIVIL. 
* it also has 360 (!) configurations with grey-grey-grey-grey-grey (_E.g._ hello / flush / lemon ...).  \

```{r cigar_grey, include=T, echo = F, fig.height=1, fig.width=5, fig.align="center", dpi = 300}

if (!file.exists('Data/colorsMat.rds')) {
  start_time <- Sys.time()
  colorsMat <- sapply(1:nrow(dictionary), function(i) { 
    sapply(1:nrow(dictionary), function(j) {
            # rows will be the guess, columns will be the target
            paste(getColors(dictionary[j,'V1'], dictionary[i,'V1'], 5), collapse = '-') 
    } ) 
    }
  )
  end_time <- Sys.time()
  print(end_time - start_time )
  
  colnames(colorsMat)<-rownames(dict)
  rownames(colorsMat)<-rownames(dict)
  saveRDS(colorsMat, 'colorsMat.rds')
}

colorsMat = readRDS('Data/colorsMat.rds')

a<-plotGuess('cigar', getColors('cigar', 'civic', 5), 5 ) +
  theme(rect = element_rect(fill = "transparent"))+ggtitle('3 possible targets')

b<-plotGuess('cigar', getColors('cigar', 'hello', 5), 5 ) +
  theme(rect = element_rect(fill = "transparent"))+ggtitle('360 possible targets')
a+b
```



Therefore if I start with the word CIGAR and get green-green-grey-grey-grey, that would be super __surprising__ (or informative) because I would be left with only 3 other words. But getting an green-green-grey-grey-grey as response is also highly __unlikely__, because it happens only with 3 words out of 2315 $\frac{3}{2315} = 0.001$.  \

__GOAL:__ I need to find a word that on average will give me highly surprising (or informative) color configurations

_I.e_ I look for a word that maximises the reduction of the space of possibilities, I want to shrink that 2315 target words dictionary into a smaller set. Sejal Dua in [her arcticle](https://towardsdatascience.com/a-deep-dive-into-wordle-the-new-pandemic-puzzle-craze-9732d97bf723) explains it well:
&nbsp;

> _We want to pick the word that yields the __largest remaining uncertainty__ (or information). This means that, regardless of if we get a bunch of green and yellow tiles or all grey tiles, we will ensure that __all possible outcomes are similar__ and that none of them are too bad._

&nbsp;

<!-- The optimal situation would be to find a word which has a unique combination of colors for every target word.  \ -->
<!-- But this is not possible, since we have many less color configurations that possible target words. So the idea is to approach the ideal optimal situation. I need to find a word for which the number of colors combination is minimum across the targets.  \ -->
&nbsp;



Here is when entropy (or information) definition becomes handy: Information theory entropy is a measure of "surprise" or "uncertanty" relatively to all the possible outcome of a random variable. Here I use "Information" and "Entropy" interchangeably.\



# Entropy
<center>
_No one really knows what entropy really is_
`r tufte::quote_footer('--- Von Neumann')`
</center>
&nbsp;

The unity measure of information is the bit. 1 Bit of information is an observation that cuts the world of possibilities in half. Or in other words, that it has $p = 0.5$ chance of occurrence. The formula for information of an event with probability $p$ is
$$
I = -log_2(p)
$$

which is equal to 1 when $p = 0.5$.
Information is just a function of the probability of a random variable, but it is useful when we need to talk about space of possibilities, which in this case is the dictionary.  In particular Information has an opposite trend with respect to probability, when an event is unlikely it has a small probability, but high information.


<center>
_The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred._
`r tufte::quote_footer('--- Deep Learning - Ian Goodfellow')`
</center>
&nbsp;

Therefore, unlikely events are also highly informative. For example, words containing the letter __E__ are `r unname(table(apply(dictionary1, 1, function(x) 'e' %in% x))['TRUE'])` out of the 2315, so they account for 45% of the dictionary. So it's really common to find a word with an E inside, and it would give about 1 ($-log2(0.45) = 1.15$) bit of information (not so much then), since it cuts in half the space of possibilities.  \
Words containing the letter __S__ are 550, so in this case this requirement halves the space of possibilities twice (550 / 2315 = 0.24), _i.e._ it splits it in 4 parts.
<!-- Words containing both the letters __E__ and __L__ are 280, so in this case this requirement halves the space of possibilities 3 times (280 / 2315 = 0.12), _i.e._ it splits it in 8 parts. -->

```{r cuts, include=T, echo = F, fig.height=3, fig.width=7, fig.align="center", dpi = 300}
col = colorRamp2(c(0,1),c('gainsboro','firebrick'))
size = 50

cuts1 = matrix(data = 0, nrow = 2, ncol = 1)
cuts1[2,1] = 1
h1 <-Heatmap(cuts1, 
             col = col,
             cluster_rows = F, cluster_columns = F,
             height              = unit(size, "mm"),
             width               = unit(size, "mm"),
             rect_gp = gpar(col = "white", lwd = 2),
             show_heatmap_legend = F,
             cell_fun = function(j, i, x, y, width, height, fill) {
               if (cuts1[i,j]==1)  {
                 grid.text('E', x, y, gp = gpar(fontsize = 15))
               }    },
             column_title = paste0('Words containing E\n1 bit of information')
)



cuts2 = matrix(data = 0, nrow = 2, ncol = 2)
cuts2[2,2] = 1
h2 <-Heatmap(cuts2, 
             col = col,
             cluster_rows = F, cluster_columns = F,
             height              = unit(size, "mm"),
             width               = unit(size, "mm"),
             rect_gp = gpar(col = "white", lwd = 2),
             show_heatmap_legend = F,
                     cell_fun = function(j, i, x, y, width, height, fill) {
          if (cuts2[i,j]==1)  {
            grid.text('S', x, y, gp = gpar(fontsize = 15))
          }    },
             column_title = paste0('Words containing S\n2 bits of information')
)

cuts3 = matrix(data = 0, nrow = 4, ncol = 2)
cuts3[4,2] = 1
h3 <-Heatmap(cuts3, 
        col = col,
        cluster_rows = F, cluster_columns = F,
        height              = unit(size, "mm"),
        width               = unit(size, "mm"),
        rect_gp = gpar(col = "white", lwd = 2),
        show_heatmap_legend = F,
        cell_fun = function(j, i, x, y, width, height, fill) {
          if (cuts3[i,j]==1)  {
            grid.text('E & L', x, y, gp = gpar(fontsize = 10))
          }    },
        column_title = paste0('Words containing E and L\n3 bits of information')
)

as.ggplot(h1)+as.ggplot(h2)+as.ggplot(h3)


```



For each guess we make, we can measure how informative each color configuration is and take the expected value of this information, which is the information of each color configuration multiplied by its probability of occuring $p_i$.

$$
I[guess] = \sum_{i}-p_i \cdot log_2(p_i)
$$
$$
i \in \{ \text{color configurations (total of 243)\}}
$$

The resulting value represents the average reduction of the dictionary after choosing the _"guess"_ word. 
Mathematically it is the expected value of information that the _"guess"_ word gives. It is an average across all the possible 2315 targets.

Basically, we loop for each pair of guess and target words, we find the 5 colors associated to it.
Then we compute the probability of having a particular color combination, and plug it into the formula.

```{r colorsMat, include=F, echo = F, eval=FALSE}
# rows will be the guesses, columns will be the targets
colorsMat <- sapply(1:nrow(dictionary), function(i) { 
  sapply(1:nrow(dictionary), function(j) {
    paste(getColors(dictionary[j,'V1'], dictionary[i,'V1'], 5), collapse = '-') 
  } ) }
)
colnames(colorsMat)<-rownames(dictionary)
rownames(colorsMat)<-rownames(dictionary)
```

<!-- https://github.com/kbroman/testhtmlpage -->


# Best and worst starting words

This heatmap shows the frequency of colors distributions for the word with highest expected value of information (the best word, RAISE), and for the word with the lowest (the worst, MAMMA).  
The height of the heatmap represents the frequency that each color configuration has, from the most common one at the bottom, to the rarest at the top.

Here you see a clear visualization of the information of the starting word. 

> The word MAMMA about half of the time would give all greys, which would only split the dictionary in half, not much of a gain. __Just in the case that the target word is among the ones at the top of the matrix MAMMA would give an high information gain__, but to have that restricted set of words as targets is really __unlikely__.

The term MAMMA is a low entropy, or low information, opening word. 

> While RAISE has a more homogeneous pattern of color configurations: __whatever the target word is, we are ensured to have an overall good shrink in the dictionary__.

RAISE is a high entropy, or high information, opening word.


```{r colors_matrix, include=T, echo = F, fig.height=5.5, fig.width=7, fig.align="center", dpi = 300}
colorsMat = readRDS('Data/colorsMat.rds')
info = getGuessesInformations(colorsMat_in = colorsMat)

best = sort(table(colorsMat[info$word[which.max(info$information)], ]), decreasing = T)
worst = sort(table(colorsMat[info$word[which.min(info$information)], ]), decreasing = T)

best_name = toupper(info$word[which.max(info$information)])
worst_name = toupper(info$word[which.min(info$information)])

to_plot = rbind(data.frame('type' = 'best', 'word' = best_name,'Frequency' = as.numeric(unname(best)), 'pos' = seq(1:length(best)), colors = names(best) ),
                data.frame('type' = 'worst', 'word' = worst_name, 'Frequency' = as.numeric(unname(worst)),'pos' = seq(1:length(worst)), colors = names(worst) ))

to_plot$word = factor(to_plot$word, levels = c(best_name, worst_name))
to_plot$type = factor(to_plot$type, levels = c('worst', 'best'))

## Plot matrix of combinations ==============
# expand each row a number of time equivalent at the Frequency column
prepare_best = with(subset(to_plot, type=='best'), rep(colors, Frequency))
prepare_worst = with(subset(to_plot, type=='worst'), rep(colors, Frequency))

best_mat = do.call(rbind, strsplit(rev(prepare_best),'-'))
worst_mat = do.call(rbind, strsplit(rev(prepare_worst),'-'))

h_best <- Heatmap(best_mat, 
                  cluster_rows = F, 
                  cluster_columns = F,
                  height              = unit(0.05, "mm")*nrow(best_mat),
                  width               = unit(5, "mm")*ncol(best_mat),
                  column_names_rot = 0,
                  show_heatmap_legend = F,
                  column_title =  best_name,
                  col = c('grey' = 'grey', 'green' = 'green3', 'yellow' = 'gold' )
)


h_worst <- Heatmap(worst_mat, 
                   cluster_rows = F, 
                   cluster_columns = F,
                   height              = unit(0.05, "mm")*nrow(best_mat),
                   width               = unit(5, "mm")*ncol(best_mat),
                   column_names_rot = 0,
                   show_heatmap_legend = F,
                   column_title = worst_name,
                   col = c('grey' = 'grey', 'green' = 'green3', 'yellow' = 'gold' ),
                   # rect_gp = gpar(col = "white", lwd = 2)
)

h = h_worst + h_best

draw(h, column_title = "All colors configurations")
```

We can visualize it in a different manner by plotting on x the all possible color configurations and on y their frequencies, or probabilities.
Looking at the shape of the two distribution you can see that __information is maximized when the probability distribution tends to uniform__, in other words when the probabilities of color configurations spread out.  \
Even if MAMMA is probably the first word we said in real life, it is the worst word to start with in WORDLE. This is because MAMMA has many repeated letters that won't give any information gain.  \

```{r best_worst, include=T, echo = F, fig.height=3, fig.width=7, fig.align="center", dpi = 300}
text = rbind( info[1,c('word','information')],
              info[nrow(info),c('word','information')])
text$word = toupper(text$word)
text$type = 'Information'

ggplot(to_plot)+geom_bar(stat='identity', position = 'dodge', aes(x = pos, y = Frequency, fill = type))+
  facet_wrap(~word)+theme_classic()+theme(axis.text.x = element_blank(), axis.title.x = element_blank())+
  ggsci::scale_fill_aaas()+
  geom_text(data = text, aes(x =max(to_plot$pos)/2, y = max(to_plot$Frequency*2/3 ), 
                             label = paste0('E(Information) = ',  round(information,2))), color = 'black')


## Plot top 10 ==========
grid = subset(to_plot, pos<=10)

best_mat_top = do.call(rbind, strsplit(subset(grid, type=='best')$colors,'-'))
worst_mat_top = do.call(rbind, strsplit(subset(grid, type=='worst')$colors,'-'))



ha = rowAnnotation(frequency = anno_barplot( subset(grid, type=='best')$Frequency  ))

h_best_top <- Heatmap(best_mat_top, 
                  right_annotation = ha,
                  cluster_rows = F, 
                  cluster_columns = F,
                  height              = unit(5, "mm")*nrow(best_mat_top),
                  width               = unit(5, "mm")*ncol(best_mat_top),
                  column_names_rot = 0,
                  show_heatmap_legend = F,
                  column_title =  best_name,
                  col = c('grey' = 'grey', 'green' = 'green3', 'yellow' = 'gold' ),
                  rect_gp = gpar(col = "white", lwd = 2)
)


ha = rowAnnotation(frequency = anno_barplot( subset(grid, type=='worst')$Frequency  ))

h_worst_top <- Heatmap(worst_mat_top, 
                   right_annotation = ha,
                   cluster_rows = F, 
                   cluster_columns = F,
                   height              = unit(5, "mm")*nrow(worst_mat_top),
                   width               = unit(5, "mm")*ncol(worst_mat_top),
                   column_names_rot = 0,
                   show_heatmap_legend = F,
                   column_title = worst_name,
                   col = c('grey' = 'grey', 'green' = 'green3', 'yellow' = 'gold' ),
                   rect_gp = gpar(col = "white", lwd = 2)
)

h_top = h_worst_top + h_best_top

draw(h_top, column_title = "Top 10 colors configurations",  ht_gap = unit(3, "cm"))

```

RAISE has an information expected value of 5.88, this means that _on average_ this guess will halve the dictionary almost 6 times, so the average dictionary size reduction will be from 2315 to $$\frac{2315}{2^{5.88}} \sim 39$$ 
For MAMMA the reduction would be from 2315 to:
$$\frac{2315}{2^{2.27}} \sim 480$$ 


# Performance
I run a simulation with 500 random target words from the list of 2315 words and using the most and the less informative word RAISE and MAMMA as first guesses.

```{r performance_entr, include=T, echo = F, fig.height=3, fig.width=8, fig.align="center", dpi = 300}
entropy_sim = readRDS('Data/entropy_first_guess_RAISE_500_simulations.RDS')
entropy_worst_sim = readRDS('Data/entropy_first_guess_MAMMA_500_simulations.RDS')

entropy_sim_df = data.frame( guesses = as.numeric(unlist(lapply(entropy_sim, function(x) 
  ifelse(length(x)>=1, dim(x)[1]+1, 1) ))), target = names(entropy_sim)  )
entropy_worst_sim_df = data.frame( guesses = as.numeric(unlist(lapply(entropy_worst_sim, function(x) 
  ifelse(length(x)>=1, dim(x)[1]+1, 1) ))), target = names(entropy_worst_sim)  )


# If the words left after the fifth guess is >1 --> Game over
failed = as.numeric(unlist(sapply(entropy_sim, function(x) {unlist(tail(x,1)[,'actual_left'])})))>1
entropy_sim_df[failed, 'guesses'] = 6

entropy_sim_df$test = ifelse(entropy_sim_df$guesses>=6, '>5', entropy_sim_df$guesses)
entropy_sim_df$test = factor(entropy_sim_df$test, levels = c('1', '2', '3', '4', '5', '>5'))
to_plot = as.data.frame(entropy_sim_df %>% dplyr::group_by(test, .drop=FALSE) %>% dplyr::summarise(n = n()))
a<-ggplot(to_plot)+
  # geom_col_pattern(aes(x = test, y = n), pattern ='placeholder', pattern_type = 'picsum', color = 'black')+
  geom_bar(aes(x = test, y = n), stat = 'identity', fill = ggsci::pal_aaas(palette = 'default')(2)[2], color = 'black')+
  theme_classic()+
  xlab('Number of guesses')+
  ggtitle(paste0('Number of lost games = ', sum(to_plot$test =='>5'),
                 '\nMean guesses in winning games = ',round(mean(entropy_sim_df$guesses[entropy_sim_df$guesses !=6]),2)))


# If the words left after the fifth guess is >1 --> Game over
failed = as.numeric(unlist(sapply(entropy_worst_sim, function(x) {unlist(tail(x,1)[,'actual_left'])})))>1
entropy_worst_sim_df[failed, 'guesses'] = 6

entropy_worst_sim_df$test = ifelse(entropy_worst_sim_df$guesses>=6, '>5', entropy_worst_sim_df$guesses)
entropy_worst_sim_df$test = factor(entropy_worst_sim_df$test, levels = c('1', '2', '3', '4', '5', '>5'))
to_plot = as.data.frame(entropy_worst_sim_df %>% dplyr::group_by(test,  .drop=FALSE) %>% dplyr::summarise(n = n()))
b<-ggplot(to_plot)+
  # geom_col_pattern(aes(x = test, y = n), pattern ='placeholder', pattern_type = 'picsum', color = 'black')+
  geom_bar(aes(x = test, y = n), stat = 'identity', fill = ggsci::pal_aaas(palette = 'default')(2)[1], color = 'black')+
  theme_classic()+
  xlab('Number of guesses')+
  ggtitle(paste0('Number of lost games = ', sum(to_plot$test =='>5'),
                 '\nMean guesses in winning games = ',round(mean(entropy_worst_sim_df$guesses[entropy_worst_sim_df$guesses !=6]),2)))

b+a

```



# Conclusions
Entropy is a mess. It is always tricky to understand and to explain concepts related to probability distributions. The main message here is that a system that has few possible configurations with heterogeneous probabilites has a low entropy (and low information). In the wordle case this system would be the game starting with the word MAMMA. while the system with many configuration with almost equal probability of occurring is a high entropy (or high information) system. This is the case of the wordle game starting with RAISE. 

<!-- Having an high entropy expected value for a system is synonym of being able to cut the space into almost equivalent parts -->


Note 1: the worst and best words in this article are different from the Andrew Steele's [video](https://www.youtube.com/watch?v=YEoCBnQwdzM) because I used only input words that are also target words, while he used all the 5 letter words in the english dictionary. This can change the probability distributions and consequently the starting words. His best word is SOARE, which is not among the target words list.


<center>
"If you guessed a coin flip correctly, how surprised would you be?   \
A bit."
</center>
&nbsp;

Explaination: a fair coin has 2 faces with equal outcome probabilities (0.5). The probability of having a head is 0.5, so you would be $-log_2( 0.5 ) = 1$ bit surprised of getting it. In other words, each of the two outcomes split the world of possibilities in half, from 2 possibilities to 1 possibility.  \

# References
* This analysis is strongly inspired by 3Blue1Brown [video](https://www.youtube.com/watch?v=v68zYyaEmEA), corrected in this [new video](https://www.youtube.com/watch?v=fRed0Xmc2Wg)  \
* Ask a mathematician, ask a physicist -  Whatâ€™s the relationship between entropy in the information-theory sense and the thermodynamics sense? [arcticle](https://www.askamathematician.com/2010/01/q-whats-the-relationship-between-entropy-in-the-information-theory-sense-and-the-thermodynamics-sense/)  \
* Claude Shannon - "A Mathematical Theory of Communication" [paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)
* Sejal Dua's "A Deep Dive into Wordle, the New Pandemic Puzzle Craze" [arcticle](https://towardsdatascience.com/a-deep-dive-into-wordle-the-new-pandemic-puzzle-craze-9732d97bf723)
* Another related video: Andrew Steele's [video](https://www.youtube.com/watch?v=YEoCBnQwdzM)
